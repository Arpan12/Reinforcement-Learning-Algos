{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fcf82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch as T\n",
    "from torch.distributions.categorical import Categorical\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class AgentNetwork(nn.Module):\n",
    "    def __init__(self,input_dims,action_dim,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n",
    "        super(AgentNetwork,self).__init__()\n",
    "        self.checkpoint_file = os.path.join(weight_file,'ppo_actor_weight')\n",
    "        #TOCHECK: *input_dims vs input_dims\n",
    "        self.actor = nn.Sequential(\n",
    "                nn.Linear(*input_dims,layer1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(layer1,layer2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(layer2,action_dim),\n",
    "                nn.Softmax(dim=-1)               \n",
    "        )\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self,state):\n",
    "        dist = self.actor(state)\n",
    "        #TOCHECK: what does categorical do\n",
    "        dist = Categorical(dist)\n",
    "        return dist\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(),self.checkpoint_file)\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self,input_dims,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n",
    "        super(CriticNetwork,self).__init__()\n",
    "        self.checkpoint_file = os.path.join(weight_file,'ppo_critic_weight')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dims,layer1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(layer1,layer2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(layer2,1)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(),self.checkpoint_file)\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9421d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self,batch_size):\n",
    "        self.states = []\n",
    "        self.probs=[]\n",
    "        self.vals=[]\n",
    "        self.actions=[]\n",
    "        self.rewards=[]\n",
    "        self.dones=[]\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def store_memory(self,state,action,prob,val,reward,done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(prob)\n",
    "        self.vals.append(val)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        \n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs=[]\n",
    "        self.vals=[]\n",
    "        self.actions=[]\n",
    "        self.rewards=[]\n",
    "        self.dones=[]\n",
    "        \n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batches=[]\n",
    "        i=0\n",
    "        indices = np.arange(n_states,dtype = np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(n_states):\n",
    "            batches.append(indices[i:i+self.batch_size])\n",
    "            i+=self.batch_size\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs), \\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards), \\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febdb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,n_actions,input_dims,gamma=0.99,lr=0.0003,lambda_factor=0.95,policy_clip=0.2,batch_size=64,n_epochs=10):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lambda_factor = lambda_factor\n",
    "        self.actor = AgentNetwork(action_dim = n_actions,input_dims = input_dims,lr = lr)\n",
    "        self.critic = CriticNetwork(input_dims,lr)\n",
    "        self.memory = ReplayMemory(batch_size)\n",
    "        \n",
    "    def remember(self,state,action,prob,val,reward,done):\n",
    "        self.memory.store_memory(state,action,prob,val,reward,done)\n",
    "    \n",
    "    def save_models(self):\n",
    "        print(\"saving model file\")\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "    \n",
    "    def load_models(self):\n",
    "        print(\"loading model file\")\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        \n",
    "    def choose_action(self,observation):\n",
    "        state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n",
    "        dist = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        #TOCHECK what is the datatype action \n",
    "        action = dist.sample()\n",
    "        #TOCHECK what does squeeze do\n",
    "        \n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "\n",
    "        \n",
    "        action = T.squeeze(action).item()\n",
    "        value  = T.squeeze(value).item()\n",
    "        return action,probs,value\n",
    "        \n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr,action_arr,probs_arr,vals_arr, \\\n",
    "            rewards_arr,dones_arr,batches = self.memory.generate_batches()\n",
    "            \n",
    "            advantages=np.zeros_like(rewards_arr)\n",
    "            \n",
    "            for t in reversed(range(len(state_arr)-1)):\n",
    "                advantages[t] = rewards_arr[t]+self.gamma*vals_arr[t+1]*(1-int(dones_arr[t]))-vals_arr[t] + self.gamma*self.lambda_factor*advantages[t+1]\n",
    "                \n",
    "            advantages = T.tensor(advantages).to(self.actor.device)\n",
    "            values = T.tensor(vals_arr).to(self.actor.device)\n",
    "            \n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch],dtype = T.float).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch],dtype = T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(probs_arr[batch],dtype = T.float).to(self.actor.device)\n",
    "                dist = self.actor(states)\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                #TOCHECK: what do exp() do\n",
    "                \n",
    "                prob_ratio = new_probs.exp()/old_probs.exp()\n",
    "                \n",
    "                weighted_prob = advantages[batch]*(prob_ratio)\n",
    "                \n",
    "                weighted_clipped_probs = T.clamp(prob_ratio,1-self.policy_clip,1+self.policy_clip)*advantages[batch]\n",
    "                \n",
    "                actor_loss = - T.min(weighted_clipped_probs,weighted_prob).mean()\n",
    "                \n",
    "                critic_values = self.critic(states)\n",
    "                #TOCHECK what does squeeze do here\n",
    "                critic_values = T.squeeze(critic_values)\n",
    "                \n",
    "                desired_state_values = advantages[batch]+values[batch]\n",
    "                critic_loss = (desired_state_values-critic_values)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss+critic_loss*0.5\n",
    "                \n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "        self.memory.clear_memory() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "print( env.observation_space.shape)\n",
    "agent = Agent(n_actions = env.action_space.n,input_dims = env.observation_space.shape,batch_size=5,n_epochs=4)\n",
    "N = 20\n",
    "episodes = 3000\n",
    "n_step=0\n",
    "learn_iters=0\n",
    "best_score = 0\n",
    "score_history=[]\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action,prob,val = agent.choose_action(state)\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        \n",
    "        score+=reward\n",
    "        n_step+=1\n",
    "        agent.remember(state,action,prob,val,reward,done)\n",
    "        state = next_state\n",
    "        if n_step %N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters+=1\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        agent.save_models()\n",
    "    print(f\"episode {ep} current score {score} avg score {avg_score} best_score {best_score}\")\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action,prob,val = agent.choose_action(state)\n",
    "        next_state,reward,done,_ = env.step(action)   \n",
    "        score+=reward\n",
    "        n_step+=1 \n",
    "        state = next_state  \n",
    "        env.render()\n",
    "    print(f\"ep score = {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
