{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8963ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48fcf82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch as T\n",
    "from torch.distributions.categorical import Categorical\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class AgentNetwork(nn.Module):\n",
    "    def __init__(self,input_dims,action_dim,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n",
    "        super(AgentNetwork,self).__init__()\n",
    "        self.checkpoint_file = os.path.join(weight_file,'ppo_actor_weight')\n",
    "        #TOCHECK: *input_dims vs input_dims\n",
    "        self.actor = nn.Sequential(\n",
    "                nn.Linear(*input_dims,layer1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(layer1,layer2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(layer2,action_dim),\n",
    "                nn.Softmax(dim=-1)               \n",
    "        )\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self,state):\n",
    "        dist = self.actor(state)\n",
    "        #TOCHECK: what does categorical do\n",
    "        dist = Categorical(dist)\n",
    "        return dist\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(),self.checkpoint_file)\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self,input_dims,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n",
    "        super(CriticNetwork,self).__init__()\n",
    "        self.checkpoint_file = os.path.join(weight_file,'ppo_critic_weight')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dims,layer1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(layer1,layer2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(layer2,1)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(),self.checkpoint_file)\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9421d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self,batch_size):\n",
    "        self.states = []\n",
    "        self.probs=[]\n",
    "        self.vals=[]\n",
    "        self.actions=[]\n",
    "        self.rewards=[]\n",
    "        self.dones=[]\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def store_memory(self,state,action,prob,val,reward,done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(prob)\n",
    "        self.vals.append(val)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        \n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs=[]\n",
    "        self.vals=[]\n",
    "        self.actions=[]\n",
    "        self.rewards=[]\n",
    "        self.dones=[]\n",
    "        \n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batches=[]\n",
    "        i=0\n",
    "        indices = np.arange(n_states,dtype = np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(n_states):\n",
    "            batches.append(indices[i:i+self.batch_size])\n",
    "            i+=self.batch_size\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs), \\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards), \\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "febdb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,n_actions,input_dims,gamma=0.99,lr=0.0003,lambda_factor=0.95,policy_clip=0.2,batch_size=64,n_epochs=10):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lambda_factor = lambda_factor\n",
    "        self.actor = AgentNetwork(action_dim = n_actions,input_dims = input_dims,lr = lr)\n",
    "        self.critic = CriticNetwork(input_dims,lr)\n",
    "        self.memory = ReplayMemory(batch_size)\n",
    "        \n",
    "    def remember(self,state,action,prob,val,reward,done):\n",
    "        self.memory.store_memory(state,action,prob,val,reward,done)\n",
    "    \n",
    "    def save_models(self):\n",
    "        print(\"saving model file\")\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "    \n",
    "    def load_models(self):\n",
    "        print(\"loading model file\")\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        \n",
    "    def choose_action(self,observation):\n",
    "        state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n",
    "        dist = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        #TOCHECK what is the datatype action \n",
    "        action = dist.sample()\n",
    "        #TOCHECK what does squeeze do\n",
    "        \n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "\n",
    "        \n",
    "        action = T.squeeze(action).item()\n",
    "        value  = T.squeeze(value).item()\n",
    "        return action,probs,value\n",
    "        \n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr,action_arr,probs_arr,vals_arr, \\\n",
    "            rewards_arr,dones_arr,batches = self.memory.generate_batches()\n",
    "            \n",
    "            advantages=np.zeros_like(rewards_arr)\n",
    "            \n",
    "            for t in reversed(range(len(state_arr)-1)):\n",
    "                advantages[t] = rewards_arr[t]+self.gamma*vals_arr[t+1]*(1-int(dones_arr[t]))-vals_arr[t] + self.gamma*self.lambda_factor*advantages[t+1]\n",
    "                \n",
    "            advantages = T.tensor(advantages).to(self.actor.device)\n",
    "            values = T.tensor(vals_arr).to(self.actor.device)\n",
    "            \n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch],dtype = T.float).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch],dtype = T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(probs_arr[batch],dtype = T.float).to(self.actor.device)\n",
    "                dist = self.actor(states)\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                #TOCHECK: what do exp() do\n",
    "                \n",
    "                prob_ratio = new_probs.exp()/old_probs.exp()\n",
    "                \n",
    "                weighted_prob = advantages[batch]*(prob_ratio)\n",
    "                \n",
    "                weighted_clipped_probs = T.clamp(prob_ratio,1-self.policy_clip,1+self.policy_clip)*advantages[batch]\n",
    "                \n",
    "                actor_loss = - T.min(weighted_clipped_probs,weighted_prob).mean()\n",
    "                \n",
    "                critic_values = self.critic(states)\n",
    "                #TOCHECK what does squeeze do here\n",
    "                critic_values = T.squeeze(critic_values)\n",
    "                \n",
    "                desired_state_values = advantages[batch]+values[batch]\n",
    "                critic_loss = (desired_state_values-critic_values)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss+critic_loss*0.5\n",
    "                \n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "        self.memory.clear_memory() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae73964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "saving model file\n",
      "episode 0 current score 11.0 avg score 11.0 best_score 11.0\n",
      "saving model file\n",
      "episode 1 current score 27.0 avg score 19.0 best_score 19.0\n",
      "saving model file\n",
      "episode 2 current score 36.0 avg score 24.666666666666668 best_score 24.666666666666668\n",
      "episode 3 current score 23.0 avg score 24.25 best_score 24.666666666666668\n",
      "episode 4 current score 20.0 avg score 23.4 best_score 24.666666666666668\n",
      "episode 5 current score 30.0 avg score 24.5 best_score 24.666666666666668\n",
      "episode 6 current score 13.0 avg score 22.857142857142858 best_score 24.666666666666668\n",
      "episode 7 current score 29.0 avg score 23.625 best_score 24.666666666666668\n",
      "saving model file\n",
      "episode 8 current score 102.0 avg score 32.333333333333336 best_score 32.333333333333336\n",
      "saving model file\n",
      "episode 9 current score 33.0 avg score 32.4 best_score 32.4\n",
      "episode 10 current score 27.0 avg score 31.90909090909091 best_score 32.4\n",
      "saving model file\n",
      "episode 11 current score 90.0 avg score 36.75 best_score 36.75\n",
      "episode 12 current score 30.0 avg score 36.23076923076923 best_score 36.75\n",
      "saving model file\n",
      "episode 13 current score 48.0 avg score 37.07142857142857 best_score 37.07142857142857\n",
      "episode 14 current score 31.0 avg score 36.666666666666664 best_score 37.07142857142857\n",
      "episode 15 current score 36.0 avg score 36.625 best_score 37.07142857142857\n",
      "saving model file\n",
      "episode 16 current score 45.0 avg score 37.11764705882353 best_score 37.11764705882353\n",
      "saving model file\n",
      "episode 17 current score 49.0 avg score 37.77777777777778 best_score 37.77777777777778\n",
      "saving model file\n",
      "episode 18 current score 45.0 avg score 38.1578947368421 best_score 38.1578947368421\n",
      "saving model file\n",
      "episode 19 current score 83.0 avg score 40.4 best_score 40.4\n",
      "saving model file\n",
      "episode 20 current score 87.0 avg score 42.61904761904762 best_score 42.61904761904762\n",
      "episode 21 current score 38.0 avg score 42.40909090909091 best_score 42.61904761904762\n",
      "saving model file\n",
      "episode 22 current score 92.0 avg score 44.56521739130435 best_score 44.56521739130435\n",
      "saving model file\n",
      "episode 23 current score 171.0 avg score 49.833333333333336 best_score 49.833333333333336\n",
      "saving model file\n",
      "episode 24 current score 109.0 avg score 52.2 best_score 52.2\n",
      "saving model file\n",
      "episode 25 current score 102.0 avg score 54.11538461538461 best_score 54.11538461538461\n",
      "saving model file\n",
      "episode 26 current score 252.0 avg score 61.44444444444444 best_score 61.44444444444444\n",
      "saving model file\n",
      "episode 27 current score 154.0 avg score 64.75 best_score 64.75\n",
      "saving model file\n",
      "episode 28 current score 76.0 avg score 65.13793103448276 best_score 65.13793103448276\n",
      "saving model file\n",
      "episode 29 current score 128.0 avg score 67.23333333333333 best_score 67.23333333333333\n",
      "saving model file\n",
      "episode 30 current score 207.0 avg score 71.74193548387096 best_score 71.74193548387096\n",
      "saving model file\n",
      "episode 31 current score 164.0 avg score 74.625 best_score 74.625\n",
      "saving model file\n",
      "episode 32 current score 264.0 avg score 80.36363636363636 best_score 80.36363636363636\n",
      "saving model file\n",
      "episode 33 current score 496.0 avg score 92.58823529411765 best_score 92.58823529411765\n",
      "episode 34 current score 82.0 avg score 92.28571428571429 best_score 92.58823529411765\n",
      "episode 35 current score 73.0 avg score 91.75 best_score 92.58823529411765\n",
      "episode 36 current score 69.0 avg score 91.13513513513513 best_score 92.58823529411765\n",
      "episode 37 current score 57.0 avg score 90.23684210526316 best_score 92.58823529411765\n",
      "episode 38 current score 21.0 avg score 88.46153846153847 best_score 92.58823529411765\n",
      "episode 39 current score 90.0 avg score 88.5 best_score 92.58823529411765\n",
      "saving model file\n",
      "episode 40 current score 303.0 avg score 93.73170731707317 best_score 93.73170731707317\n",
      "saving model file\n",
      "episode 41 current score 229.0 avg score 96.95238095238095 best_score 96.95238095238095\n",
      "saving model file\n",
      "episode 42 current score 500.0 avg score 106.32558139534883 best_score 106.32558139534883\n",
      "episode 43 current score 56.0 avg score 105.18181818181819 best_score 106.32558139534883\n",
      "episode 44 current score 28.0 avg score 103.46666666666667 best_score 106.32558139534883\n",
      "episode 45 current score 65.0 avg score 102.6304347826087 best_score 106.32558139534883\n",
      "episode 46 current score 127.0 avg score 103.14893617021276 best_score 106.32558139534883\n",
      "episode 47 current score 144.0 avg score 104.0 best_score 106.32558139534883\n",
      "episode 48 current score 149.0 avg score 104.91836734693878 best_score 106.32558139534883\n",
      "episode 49 current score 170.0 avg score 106.22 best_score 106.32558139534883\n",
      "saving model file\n",
      "episode 50 current score 221.0 avg score 108.47058823529412 best_score 108.47058823529412\n",
      "saving model file\n",
      "episode 51 current score 220.0 avg score 110.61538461538461 best_score 110.61538461538461\n",
      "saving model file\n",
      "episode 52 current score 377.0 avg score 115.64150943396227 best_score 115.64150943396227\n",
      "saving model file\n",
      "episode 53 current score 376.0 avg score 120.46296296296296 best_score 120.46296296296296\n",
      "saving model file\n",
      "episode 54 current score 209.0 avg score 122.07272727272728 best_score 122.07272727272728\n",
      "saving model file\n",
      "episode 55 current score 173.0 avg score 122.98214285714286 best_score 122.98214285714286\n",
      "saving model file\n",
      "episode 56 current score 184.0 avg score 124.05263157894737 best_score 124.05263157894737\n",
      "saving model file\n",
      "episode 57 current score 219.0 avg score 125.6896551724138 best_score 125.6896551724138\n",
      "saving model file\n",
      "episode 58 current score 134.0 avg score 125.83050847457628 best_score 125.83050847457628\n",
      "episode 59 current score 115.0 avg score 125.65 best_score 125.83050847457628\n",
      "saving model file\n",
      "episode 60 current score 235.0 avg score 127.44262295081967 best_score 127.44262295081967\n",
      "saving model file\n",
      "episode 61 current score 178.0 avg score 128.25806451612902 best_score 128.25806451612902\n",
      "saving model file\n",
      "episode 62 current score 186.0 avg score 129.17460317460316 best_score 129.17460317460316\n",
      "saving model file\n",
      "episode 63 current score 155.0 avg score 129.578125 best_score 129.578125\n",
      "saving model file\n",
      "episode 64 current score 280.0 avg score 131.8923076923077 best_score 131.8923076923077\n",
      "episode 65 current score 43.0 avg score 130.54545454545453 best_score 131.8923076923077\n",
      "episode 66 current score 35.0 avg score 129.11940298507463 best_score 131.8923076923077\n",
      "episode 67 current score 12.0 avg score 127.3970588235294 best_score 131.8923076923077\n",
      "episode 68 current score 32.0 avg score 126.01449275362319 best_score 131.8923076923077\n",
      "episode 69 current score 12.0 avg score 124.38571428571429 best_score 131.8923076923077\n",
      "episode 70 current score 12.0 avg score 122.80281690140845 best_score 131.8923076923077\n",
      "episode 71 current score 10.0 avg score 121.23611111111111 best_score 131.8923076923077\n",
      "episode 72 current score 10.0 avg score 119.71232876712328 best_score 131.8923076923077\n",
      "episode 73 current score 10.0 avg score 118.22972972972973 best_score 131.8923076923077\n",
      "episode 74 current score 10.0 avg score 116.78666666666666 best_score 131.8923076923077\n",
      "episode 75 current score 11.0 avg score 115.39473684210526 best_score 131.8923076923077\n",
      "episode 76 current score 11.0 avg score 114.03896103896103 best_score 131.8923076923077\n",
      "episode 77 current score 8.0 avg score 112.67948717948718 best_score 131.8923076923077\n",
      "episode 78 current score 10.0 avg score 111.37974683544304 best_score 131.8923076923077\n",
      "episode 79 current score 9.0 avg score 110.1 best_score 131.8923076923077\n",
      "episode 80 current score 11.0 avg score 108.87654320987654 best_score 131.8923076923077\n",
      "episode 81 current score 9.0 avg score 107.65853658536585 best_score 131.8923076923077\n",
      "episode 82 current score 10.0 avg score 106.48192771084338 best_score 131.8923076923077\n",
      "episode 83 current score 9.0 avg score 105.32142857142857 best_score 131.8923076923077\n",
      "episode 84 current score 11.0 avg score 104.21176470588236 best_score 131.8923076923077\n",
      "episode 85 current score 10.0 avg score 103.11627906976744 best_score 131.8923076923077\n",
      "episode 86 current score 11.0 avg score 102.05747126436782 best_score 131.8923076923077\n",
      "episode 87 current score 10.0 avg score 101.01136363636364 best_score 131.8923076923077\n",
      "episode 88 current score 10.0 avg score 99.98876404494382 best_score 131.8923076923077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 89 current score 28.0 avg score 99.18888888888888 best_score 131.8923076923077\n",
      "episode 90 current score 73.0 avg score 98.9010989010989 best_score 131.8923076923077\n",
      "episode 91 current score 213.0 avg score 100.1413043478261 best_score 131.8923076923077\n",
      "episode 92 current score 135.0 avg score 100.51612903225806 best_score 131.8923076923077\n",
      "episode 93 current score 126.0 avg score 100.7872340425532 best_score 131.8923076923077\n",
      "episode 94 current score 174.0 avg score 101.5578947368421 best_score 131.8923076923077\n",
      "episode 95 current score 178.0 avg score 102.35416666666667 best_score 131.8923076923077\n",
      "episode 96 current score 156.0 avg score 102.90721649484536 best_score 131.8923076923077\n",
      "episode 97 current score 184.0 avg score 103.73469387755102 best_score 131.8923076923077\n",
      "episode 98 current score 236.0 avg score 105.07070707070707 best_score 131.8923076923077\n",
      "episode 99 current score 242.0 avg score 106.44 best_score 131.8923076923077\n",
      "episode 100 current score 198.0 avg score 108.31 best_score 131.8923076923077\n",
      "episode 101 current score 165.0 avg score 109.69 best_score 131.8923076923077\n",
      "episode 102 current score 500.0 avg score 114.33 best_score 131.8923076923077\n",
      "episode 103 current score 141.0 avg score 115.51 best_score 131.8923076923077\n",
      "episode 104 current score 165.0 avg score 116.96 best_score 131.8923076923077\n",
      "episode 105 current score 103.0 avg score 117.69 best_score 131.8923076923077\n",
      "episode 106 current score 145.0 avg score 119.01 best_score 131.8923076923077\n",
      "episode 107 current score 158.0 avg score 120.3 best_score 131.8923076923077\n",
      "episode 108 current score 134.0 avg score 120.62 best_score 131.8923076923077\n",
      "episode 109 current score 152.0 avg score 121.81 best_score 131.8923076923077\n",
      "episode 110 current score 151.0 avg score 123.05 best_score 131.8923076923077\n",
      "episode 111 current score 238.0 avg score 124.53 best_score 131.8923076923077\n",
      "episode 112 current score 216.0 avg score 126.39 best_score 131.8923076923077\n",
      "episode 113 current score 239.0 avg score 128.3 best_score 131.8923076923077\n",
      "episode 114 current score 184.0 avg score 129.83 best_score 131.8923076923077\n",
      "episode 115 current score 201.0 avg score 131.48 best_score 131.8923076923077\n",
      "episode 116 current score 81.0 avg score 131.84 best_score 131.8923076923077\n",
      "saving model file\n",
      "episode 117 current score 107.0 avg score 132.42 best_score 132.42\n",
      "saving model file\n",
      "episode 118 current score 119.0 avg score 133.16 best_score 133.16\n",
      "episode 119 current score 68.0 avg score 133.01 best_score 133.16\n",
      "saving model file\n",
      "episode 120 current score 183.0 avg score 133.97 best_score 133.97\n",
      "saving model file\n",
      "episode 121 current score 102.0 avg score 134.61 best_score 134.61\n",
      "saving model file\n",
      "episode 122 current score 114.0 avg score 134.83 best_score 134.83\n",
      "episode 123 current score 108.0 avg score 134.2 best_score 134.83\n",
      "episode 124 current score 151.0 avg score 134.62 best_score 134.83\n",
      "saving model file\n",
      "episode 125 current score 278.0 avg score 136.38 best_score 136.38\n",
      "episode 126 current score 239.0 avg score 136.25 best_score 136.38\n",
      "saving model file\n",
      "episode 127 current score 243.0 avg score 137.14 best_score 137.14\n",
      "saving model file\n",
      "episode 128 current score 273.0 avg score 139.11 best_score 139.11\n",
      "saving model file\n",
      "episode 129 current score 268.0 avg score 140.51 best_score 140.51\n",
      "saving model file\n",
      "episode 130 current score 500.0 avg score 143.44 best_score 143.44\n",
      "saving model file\n",
      "episode 131 current score 192.0 avg score 143.72 best_score 143.72\n",
      "episode 132 current score 135.0 avg score 142.43 best_score 143.72\n",
      "episode 133 current score 8.0 avg score 137.55 best_score 143.72\n",
      "episode 134 current score 13.0 avg score 136.86 best_score 143.72\n",
      "episode 135 current score 17.0 avg score 136.3 best_score 143.72\n",
      "episode 136 current score 13.0 avg score 135.74 best_score 143.72\n",
      "episode 137 current score 244.0 avg score 137.61 best_score 143.72\n",
      "episode 138 current score 402.0 avg score 141.42 best_score 143.72\n",
      "episode 139 current score 265.0 avg score 143.17 best_score 143.72\n",
      "saving model file\n",
      "episode 140 current score 500.0 avg score 145.14 best_score 145.14\n",
      "saving model file\n",
      "episode 141 current score 500.0 avg score 147.85 best_score 147.85\n",
      "episode 142 current score 429.0 avg score 147.14 best_score 147.85\n",
      "saving model file\n",
      "episode 143 current score 500.0 avg score 151.58 best_score 151.58\n",
      "saving model file\n",
      "episode 144 current score 177.0 avg score 153.07 best_score 153.07\n",
      "saving model file\n",
      "episode 145 current score 500.0 avg score 157.42 best_score 157.42\n",
      "saving model file\n",
      "episode 146 current score 500.0 avg score 161.15 best_score 161.15\n",
      "saving model file\n",
      "episode 147 current score 500.0 avg score 164.71 best_score 164.71\n",
      "saving model file\n",
      "episode 148 current score 196.0 avg score 165.18 best_score 165.18\n",
      "episode 149 current score 10.0 avg score 163.58 best_score 165.18\n",
      "episode 150 current score 10.0 avg score 161.47 best_score 165.18\n",
      "episode 151 current score 163.0 avg score 160.9 best_score 165.18\n",
      "episode 152 current score 250.0 avg score 159.63 best_score 165.18\n",
      "episode 153 current score 276.0 avg score 158.63 best_score 165.18\n",
      "episode 154 current score 239.0 avg score 158.93 best_score 165.18\n",
      "episode 155 current score 491.0 avg score 162.11 best_score 165.18\n",
      "episode 156 current score 346.0 avg score 163.73 best_score 165.18\n",
      "episode 157 current score 135.0 avg score 162.89 best_score 165.18\n",
      "episode 158 current score 10.0 avg score 161.65 best_score 165.18\n",
      "episode 159 current score 181.0 avg score 162.31 best_score 165.18\n",
      "episode 160 current score 281.0 avg score 162.77 best_score 165.18\n",
      "episode 161 current score 243.0 avg score 163.42 best_score 165.18\n",
      "saving model file\n",
      "episode 162 current score 500.0 avg score 166.56 best_score 166.56\n",
      "saving model file\n",
      "episode 163 current score 346.0 avg score 168.47 best_score 168.47\n",
      "saving model file\n",
      "episode 164 current score 455.0 avg score 170.22 best_score 170.22\n",
      "saving model file\n",
      "episode 165 current score 410.0 avg score 173.89 best_score 173.89\n",
      "saving model file\n",
      "episode 166 current score 351.0 avg score 177.05 best_score 177.05\n",
      "saving model file\n",
      "episode 167 current score 277.0 avg score 179.7 best_score 179.7\n",
      "saving model file\n",
      "episode 168 current score 282.0 avg score 182.2 best_score 182.2\n",
      "saving model file\n",
      "episode 169 current score 256.0 avg score 184.64 best_score 184.64\n",
      "saving model file\n",
      "episode 170 current score 500.0 avg score 189.52 best_score 189.52\n",
      "saving model file\n",
      "episode 171 current score 300.0 avg score 192.42 best_score 192.42\n",
      "saving model file\n",
      "episode 172 current score 500.0 avg score 197.32 best_score 197.32\n",
      "saving model file\n",
      "episode 173 current score 500.0 avg score 202.22 best_score 202.22\n",
      "saving model file\n",
      "episode 174 current score 500.0 avg score 207.12 best_score 207.12\n",
      "saving model file\n",
      "episode 175 current score 293.0 avg score 209.94 best_score 209.94\n",
      "saving model file\n",
      "episode 176 current score 367.0 avg score 213.5 best_score 213.5\n",
      "saving model file\n",
      "episode 177 current score 500.0 avg score 218.42 best_score 218.42\n",
      "saving model file\n",
      "episode 178 current score 194.0 avg score 220.26 best_score 220.26\n",
      "saving model file\n",
      "episode 179 current score 102.0 avg score 221.19 best_score 221.19\n",
      "saving model file\n",
      "episode 180 current score 374.0 avg score 224.82 best_score 224.82\n",
      "saving model file\n",
      "episode 181 current score 336.0 avg score 228.09 best_score 228.09\n",
      "saving model file\n",
      "episode 182 current score 335.0 avg score 231.34 best_score 231.34\n",
      "saving model file\n",
      "episode 183 current score 86.0 avg score 232.11 best_score 232.11\n",
      "saving model file\n",
      "episode 184 current score 51.0 avg score 232.51 best_score 232.51\n",
      "saving model file\n",
      "episode 185 current score 208.0 avg score 234.49 best_score 234.49\n",
      "saving model file\n",
      "episode 186 current score 321.0 avg score 237.59 best_score 237.59\n",
      "saving model file\n",
      "episode 187 current score 395.0 avg score 241.44 best_score 241.44\n",
      "saving model file\n",
      "episode 188 current score 207.0 avg score 243.41 best_score 243.41\n",
      "saving model file\n",
      "episode 189 current score 289.0 avg score 246.02 best_score 246.02\n",
      "saving model file\n",
      "episode 190 current score 500.0 avg score 250.29 best_score 250.29\n",
      "episode 191 current score 25.0 avg score 248.41 best_score 250.29\n",
      "episode 192 current score 20.0 avg score 247.26 best_score 250.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 193 current score 10.0 avg score 246.1 best_score 250.29\n",
      "episode 194 current score 12.0 avg score 244.48 best_score 250.29\n",
      "episode 195 current score 33.0 avg score 243.03 best_score 250.29\n",
      "episode 196 current score 35.0 avg score 241.82 best_score 250.29\n",
      "episode 197 current score 105.0 avg score 241.03 best_score 250.29\n",
      "episode 198 current score 353.0 avg score 242.2 best_score 250.29\n",
      "episode 199 current score 370.0 avg score 243.48 best_score 250.29\n",
      "episode 200 current score 311.0 avg score 244.61 best_score 250.29\n",
      "episode 201 current score 500.0 avg score 247.96 best_score 250.29\n",
      "episode 202 current score 500.0 avg score 247.96 best_score 250.29\n",
      "saving model file\n",
      "episode 203 current score 500.0 avg score 251.55 best_score 251.55\n",
      "saving model file\n",
      "episode 204 current score 500.0 avg score 254.9 best_score 254.9\n",
      "episode 205 current score 101.0 avg score 254.88 best_score 254.9\n",
      "saving model file\n",
      "episode 206 current score 428.0 avg score 257.71 best_score 257.71\n",
      "saving model file\n",
      "episode 207 current score 500.0 avg score 261.13 best_score 261.13\n",
      "saving model file\n",
      "episode 208 current score 500.0 avg score 264.79 best_score 264.79\n",
      "episode 209 current score 28.0 avg score 263.55 best_score 264.79\n",
      "saving model file\n",
      "episode 210 current score 500.0 avg score 267.04 best_score 267.04\n",
      "saving model file\n",
      "episode 211 current score 500.0 avg score 269.66 best_score 269.66\n",
      "saving model file\n",
      "episode 212 current score 500.0 avg score 272.5 best_score 272.5\n",
      "saving model file\n",
      "episode 213 current score 500.0 avg score 275.11 best_score 275.11\n",
      "saving model file\n",
      "episode 214 current score 500.0 avg score 278.27 best_score 278.27\n",
      "saving model file\n",
      "episode 215 current score 500.0 avg score 281.26 best_score 281.26\n",
      "saving model file\n",
      "episode 216 current score 500.0 avg score 285.45 best_score 285.45\n",
      "saving model file\n",
      "episode 217 current score 500.0 avg score 289.38 best_score 289.38\n",
      "saving model file\n",
      "episode 218 current score 500.0 avg score 293.19 best_score 293.19\n",
      "saving model file\n",
      "episode 219 current score 500.0 avg score 297.51 best_score 297.51\n",
      "saving model file\n",
      "episode 220 current score 500.0 avg score 300.68 best_score 300.68\n",
      "saving model file\n",
      "episode 221 current score 500.0 avg score 304.66 best_score 304.66\n",
      "saving model file\n",
      "episode 222 current score 250.0 avg score 306.02 best_score 306.02\n",
      "saving model file\n",
      "episode 223 current score 500.0 avg score 309.94 best_score 309.94\n",
      "saving model file\n",
      "episode 224 current score 500.0 avg score 313.43 best_score 313.43\n",
      "saving model file\n",
      "episode 225 current score 500.0 avg score 315.65 best_score 315.65\n",
      "episode 226 current score 120.0 avg score 314.46 best_score 315.65\n",
      "episode 227 current score 16.0 avg score 312.19 best_score 315.65\n",
      "episode 228 current score 12.0 avg score 309.58 best_score 315.65\n",
      "episode 229 current score 25.0 avg score 307.15 best_score 315.65\n",
      "episode 230 current score 29.0 avg score 302.44 best_score 315.65\n",
      "episode 231 current score 98.0 avg score 301.5 best_score 315.65\n",
      "episode 232 current score 394.0 avg score 304.09 best_score 315.65\n",
      "episode 233 current score 500.0 avg score 309.01 best_score 315.65\n",
      "episode 234 current score 500.0 avg score 313.88 best_score 315.65\n",
      "saving model file\n",
      "episode 235 current score 500.0 avg score 318.71 best_score 318.71\n",
      "saving model file\n",
      "episode 236 current score 153.0 avg score 320.11 best_score 320.11\n",
      "saving model file\n",
      "episode 237 current score 500.0 avg score 322.67 best_score 322.67\n",
      "saving model file\n",
      "episode 238 current score 500.0 avg score 323.65 best_score 323.65\n",
      "saving model file\n",
      "episode 239 current score 500.0 avg score 326.0 best_score 326.0\n",
      "episode 240 current score 480.0 avg score 325.8 best_score 326.0\n",
      "episode 241 current score 380.0 avg score 324.6 best_score 326.0\n",
      "episode 242 current score 311.0 avg score 323.42 best_score 326.0\n",
      "episode 243 current score 267.0 avg score 321.09 best_score 326.0\n",
      "episode 244 current score 319.0 avg score 322.51 best_score 326.0\n",
      "episode 245 current score 410.0 avg score 321.61 best_score 326.0\n",
      "episode 246 current score 500.0 avg score 321.61 best_score 326.0\n",
      "episode 247 current score 386.0 avg score 320.47 best_score 326.0\n",
      "episode 248 current score 237.0 avg score 320.88 best_score 326.0\n",
      "episode 249 current score 279.0 avg score 323.57 best_score 326.0\n",
      "episode 250 current score 252.0 avg score 325.99 best_score 326.0\n",
      "saving model file\n",
      "episode 251 current score 222.0 avg score 326.58 best_score 326.58\n",
      "saving model file\n",
      "episode 252 current score 258.0 avg score 326.66 best_score 326.66\n",
      "episode 253 current score 242.0 avg score 326.32 best_score 326.66\n",
      "episode 254 current score 199.0 avg score 325.92 best_score 326.66\n",
      "episode 255 current score 268.0 avg score 323.69 best_score 326.66\n",
      "episode 256 current score 302.0 avg score 323.25 best_score 326.66\n",
      "episode 257 current score 286.0 avg score 324.76 best_score 326.66\n",
      "saving model file\n",
      "episode 258 current score 275.0 avg score 327.41 best_score 327.41\n",
      "saving model file\n",
      "episode 259 current score 243.0 avg score 328.03 best_score 328.03\n",
      "episode 260 current score 249.0 avg score 327.71 best_score 328.03\n",
      "saving model file\n",
      "episode 261 current score 306.0 avg score 328.34 best_score 328.34\n",
      "episode 262 current score 249.0 avg score 325.83 best_score 328.34\n",
      "episode 263 current score 172.0 avg score 324.09 best_score 328.34\n",
      "episode 264 current score 124.0 avg score 320.78 best_score 328.34\n",
      "episode 265 current score 124.0 avg score 317.92 best_score 328.34\n",
      "episode 266 current score 116.0 avg score 315.57 best_score 328.34\n",
      "episode 267 current score 112.0 avg score 313.92 best_score 328.34\n",
      "episode 268 current score 112.0 avg score 312.22 best_score 328.34\n",
      "episode 269 current score 119.0 avg score 310.85 best_score 328.34\n",
      "episode 270 current score 137.0 avg score 307.22 best_score 328.34\n",
      "episode 271 current score 141.0 avg score 305.63 best_score 328.34\n",
      "episode 272 current score 139.0 avg score 302.02 best_score 328.34\n",
      "episode 273 current score 147.0 avg score 298.49 best_score 328.34\n",
      "episode 274 current score 174.0 avg score 295.23 best_score 328.34\n",
      "episode 275 current score 167.0 avg score 293.97 best_score 328.34\n",
      "episode 276 current score 172.0 avg score 292.02 best_score 328.34\n",
      "episode 277 current score 148.0 avg score 288.5 best_score 328.34\n",
      "episode 278 current score 183.0 avg score 288.39 best_score 328.34\n",
      "episode 279 current score 194.0 avg score 289.31 best_score 328.34\n",
      "episode 280 current score 198.0 avg score 287.55 best_score 328.34\n",
      "episode 281 current score 196.0 avg score 286.15 best_score 328.34\n",
      "episode 282 current score 221.0 avg score 285.01 best_score 328.34\n",
      "episode 283 current score 346.0 avg score 287.61 best_score 328.34\n",
      "episode 284 current score 347.0 avg score 290.57 best_score 328.34\n",
      "episode 285 current score 376.0 avg score 292.25 best_score 328.34\n",
      "episode 286 current score 150.0 avg score 290.54 best_score 328.34\n",
      "episode 287 current score 422.0 avg score 290.81 best_score 328.34\n",
      "episode 288 current score 500.0 avg score 293.74 best_score 328.34\n",
      "episode 289 current score 500.0 avg score 295.85 best_score 328.34\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_step \u001b[38;5;241m%\u001b[39mN \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m         learn_iters\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     29\u001b[0m score_history\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "Cell \u001b[0;32mIn[7], line 57\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m old_probs \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mtensor(probs_arr[batch],dtype \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     56\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(states)\n\u001b[0;32m---> 57\u001b[0m new_probs \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#TOCHECK: what do exp() do\u001b[39;00m\n\u001b[1;32m     60\u001b[0m prob_ratio \u001b[38;5;241m=\u001b[39m new_probs\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m/\u001b[39mold_probs\u001b[38;5;241m.\u001b[39mexp()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/distributions/categorical.py:124\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[0;32m--> 124\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    125\u001b[0m value, log_pmf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits)\n\u001b[1;32m    126\u001b[0m value \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "print( env.observation_space.shape)\n",
    "agent = Agent(n_actions = env.action_space.n,input_dims = env.observation_space.shape,batch_size=5,n_epochs=4)\n",
    "N = 20\n",
    "episodes = 3000\n",
    "n_step=0\n",
    "learn_iters=0\n",
    "best_score = 0\n",
    "score_history=[]\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action,prob,val = agent.choose_action(state)\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        \n",
    "        score+=reward\n",
    "        n_step+=1\n",
    "        agent.remember(state,action,prob,val,reward,done)\n",
    "        state = next_state\n",
    "        if n_step %N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters+=1\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        agent.save_models()\n",
    "    print(f\"episode {ep} current score {score} avg score {avg_score} best_score {best_score}\")\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a4e5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep score = 425.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 416.0\n",
      "ep score = 500.0\n",
      "ep score = 462.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 393.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 431.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n"
     ]
    }
   ],
   "source": [
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action,prob,val = agent.choose_action(state)\n",
    "        next_state,reward,done,_ = env.step(action)   \n",
    "        score+=reward\n",
    "        n_step+=1 \n",
    "        state = next_state  \n",
    "        env.render()\n",
    "    print(f\"ep score = {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
