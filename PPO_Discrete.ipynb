{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8963ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48fcf82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch as T\n",
    "from torch.distributions.categorical import Categorical\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class AgentNetwork(nn.Module):\n",
    "    def __init__(self,input_dims,action_dim,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n",
    "        super(AgentNetwork,self).__init__()\n",
    "        self.checkpoint_file = os.path.join(weight_file,'ppo_actor_weight')\n",
    "        #TOCHECK: *input_dims vs input_dims\n",
    "        self.actor = nn.Sequential(\n",
    "                nn.Linear(*input_dims,layer1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(layer1,layer2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(layer2,action_dim),\n",
    "                nn.Softmax(dim=-1)               \n",
    "        )\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self,state):\n",
    "        dist = self.actor(state)\n",
    "        #TOCHECK: what does categorical do\n",
    "        dist = Categorical(dist)\n",
    "        return dist\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(),self.checkpoint_file)\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self,input_dims,lr,layer1=256,layer2=256,weight_file='weightFiles/ppo_discrete'):\n",
    "        super(CriticNetwork,self).__init__()\n",
    "        self.checkpoint_file = os.path.join(weight_file,'ppo_critic_weight')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dims,layer1),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(layer1,layer2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(layer2,1)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(),self.checkpoint_file)\n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9421d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self,batch_size):\n",
    "        self.states = []\n",
    "        self.probs=[]\n",
    "        self.vals=[]\n",
    "        self.actions=[]\n",
    "        self.rewards=[]\n",
    "        self.dones=[]\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def store_memory(self,state,action,prob,val,reward,done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(prob)\n",
    "        self.vals.append(val)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        \n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs=[]\n",
    "        self.vals=[]\n",
    "        self.actions=[]\n",
    "        self.rewards=[]\n",
    "        self.dones=[]\n",
    "        \n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batches=[]\n",
    "        i=0\n",
    "        indices = np.arange(n_states,dtype = np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(n_states):\n",
    "            batches.append(indices[i:i+self.batch_size])\n",
    "            i+=self.batch_size\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs), \\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards), \\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "febdb4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,n_actions,input_dims,gamma=0.99,lr=0.0003,lambda_factor=0.95,policy_clip=0.2,batch_size=64,n_epochs=10):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lambda_factor = lambda_factor\n",
    "        self.actor = AgentNetwork(action_dim = n_actions,input_dims = input_dims,lr = lr)\n",
    "        self.critic = CriticNetwork(input_dims,lr)\n",
    "        self.memory = ReplayMemory(batch_size)\n",
    "        \n",
    "    def remember(self,state,action,prob,val,reward,done):\n",
    "        self.memory.store_memory(state,action,prob,val,reward,done)\n",
    "    \n",
    "    def save_models(self):\n",
    "        print(\"saving model file\")\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "    \n",
    "    def load_models(self):\n",
    "        print(\"loading model file\")\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        \n",
    "    def choose_action(self,observation):\n",
    "        state = T.tensor([observation],dtype=T.float).to(self.actor.device)\n",
    "        dist = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        print(f\"value {value}\")\n",
    "        #TOCHECK what is the datatype action \n",
    "        action = dist.sample()\n",
    "        print(f\"action = {action}\")\n",
    "        #TOCHECK what does squeeze do\n",
    "        \n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "        print(f\"log prob {dist.log_prob(action)} squeezed = {T.squeeze(dist.log_prob(action))} Probs = {probs}\")\n",
    "        action = T.squeeze(action).item()\n",
    "        value  = T.squeeze(value).item()\n",
    "        return action,probs,value\n",
    "        \n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr,action_arr,probs_arr,vals_arr, \\\n",
    "            rewards_arr,dones_arr,batches = self.memory.generate_batches()\n",
    "            \n",
    "            advantages=np.zeros_like(rewards_arr)\n",
    "            \n",
    "            for t in reversed(range(len(state_arr)-1)):\n",
    "                advantages[t] = rewards_arr[t]+self.gamma*vals_arr[t+1]*(1-int(dones_arr[t]))-vals_arr[t] + self.gamma*self.lambda_factor*advantages[t+1]\n",
    "                \n",
    "            advantages = T.tensor(advantages).to(self.actor.device)\n",
    "            values = T.tensor(vals_arr).to(self.actor.device)\n",
    "            \n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch],dtype = T.float).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch],dtype = T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(probs_arr[batch],dtype = T.float).to(self.actor.device)\n",
    "                dist = self.actor(states)\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                #TOCHECK: what do exp() do\n",
    "                \n",
    "                prob_ratio = new_probs.exp()/old_probs.exp()\n",
    "                \n",
    "                weighted_prob = advantages[batch]*(prob_ratio)\n",
    "                \n",
    "                weighted_clipped_probs = T.clamp(prob_ratio,1-self.policy_clip,1+self.policy_clip)*advantages[batch]\n",
    "                \n",
    "                actor_loss = - T.min(weighted_clipped_probs,weighted_prob).mean()\n",
    "                \n",
    "                critic_values = self.critic(states)\n",
    "                #TOCHECK what does squeeze do here\n",
    "                critic_values = T.squeeze(critic_values)\n",
    "                \n",
    "                desired_state_values = advantages[batch]+values[batch]\n",
    "                critic_loss = (desired_state_values-critic_values)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "                \n",
    "                total_loss = actor_loss+critic_loss*0.5\n",
    "                \n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "        self.memory.clear_memory() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae73964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "value tensor([[0.0229]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([1], device='cuda:0')\n",
      "log prob tensor([-0.6421], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.6421285271644592 Probs = -0.6421285271644592\n",
      "value tensor([[-0.0144]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([0], device='cuda:0')\n",
      "log prob tensor([-0.7557], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.7556757926940918 Probs = -0.7556757926940918\n",
      "value tensor([[0.0235]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([1], device='cuda:0')\n",
      "log prob tensor([-0.6425], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.6424530148506165 Probs = -0.6424530148506165\n",
      "value tensor([[-0.0142]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([0], device='cuda:0')\n",
      "log prob tensor([-0.7555], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.7555360198020935 Probs = -0.7555360198020935\n",
      "value tensor([[0.0235]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([0], device='cuda:0')\n",
      "log prob tensor([-0.7464], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.7463958263397217 Probs = -0.7463958263397217\n",
      "value tensor([[0.0502]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([0], device='cuda:0')\n",
      "log prob tensor([-0.7283], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.7282710671424866 Probs = -0.7282710671424866\n",
      "value tensor([[0.0652]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([1], device='cuda:0')\n",
      "log prob tensor([-0.6863], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.6862819194793701 Probs = -0.6862819194793701\n",
      "value tensor([[0.0495]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([1], device='cuda:0')\n",
      "log prob tensor([-0.6585], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.6585426330566406 Probs = -0.6585426330566406\n",
      "value tensor([[0.0222]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([0], device='cuda:0')\n",
      "log prob tensor([-0.7474], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.7474324107170105 Probs = -0.7474324107170105\n",
      "value tensor([[0.0496]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([1], device='cuda:0')\n",
      "log prob tensor([-0.6586], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.6585809588432312 Probs = -0.6585809588432312\n",
      "value tensor([[0.0226]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([1], device='cuda:0')\n",
      "log prob tensor([-0.6418], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.6417911052703857 Probs = -0.6417911052703857\n",
      "value tensor([[-0.0144]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([1], device='cuda:0')\n",
      "log prob tensor([-0.6341], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.6340674757957458 Probs = -0.6340674757957458\n",
      "value tensor([[-0.0411]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([0], device='cuda:0')\n",
      "log prob tensor([-0.7533], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.753328263759613 Probs = -0.753328263759613\n",
      "value tensor([[-0.0131]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([1], device='cuda:0')\n",
      "log prob tensor([-0.6345], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.6345087289810181 Probs = -0.6345087289810181\n",
      "value tensor([[-0.0405]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([1], device='cuda:0')\n",
      "log prob tensor([-0.6367], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.6366772055625916 Probs = -0.6366772055625916\n",
      "value tensor([[-0.0549]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([1], device='cuda:0')\n",
      "log prob tensor([-0.6426], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.6426464319229126 Probs = -0.6426464319229126\n",
      "value tensor([[-0.0600]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([1], device='cuda:0')\n",
      "log prob tensor([-0.6455], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.645546019077301 Probs = -0.645546019077301\n",
      "value tensor([[-0.0662]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([1], device='cuda:0')\n",
      "log prob tensor([-0.6467], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.6466891169548035 Probs = -0.6466891169548035\n",
      "value tensor([[-0.0663]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([1], device='cuda:0')\n",
      "log prob tensor([-0.6495], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.6495181322097778 Probs = -0.6495181322097778\n",
      "value tensor([[-0.0674]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "action = tensor([1], device='cuda:0')\n",
      "log prob tensor([-0.6520], device='cuda:0', grad_fn=<SqueezeBackward1>) squeezed = -0.65202796459198 Probs = -0.65202796459198\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_step \u001b[38;5;241m%\u001b[39mN \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m         learn_iters\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     29\u001b[0m score_history\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "Cell \u001b[0;32mIn[17], line 43\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs):\n\u001b[1;32m     42\u001b[0m         state_arr,action_arr,probs_arr,vals_arr, \\\n\u001b[0;32m---> 43\u001b[0m         rewards_arr,dones_arr,batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m         advantages\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros_like(rewards_arr)\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(state_arr)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m, in \u001b[0;36mReplayMemory.generate_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m     batches\u001b[38;5;241m.\u001b[39mappend(indices[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size])\n\u001b[1;32m     35\u001b[0m     i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates),\\\n\u001b[1;32m     37\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions),\\\n\u001b[0;32m---> 38\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m, \\\n\u001b[1;32m     39\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvals),\\\n\u001b[1;32m     40\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards), \\\n\u001b[1;32m     41\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdones),\\\n\u001b[1;32m     42\u001b[0m         batches\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:956\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 956\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "print( env.observation_space.shape)\n",
    "agent = Agent(n_actions = env.action_space.n,input_dims = env.observation_space.shape,batch_size=5,n_epochs=4)\n",
    "N = 20\n",
    "episodes = 3000\n",
    "n_step=0\n",
    "learn_iters=0\n",
    "best_score = 0\n",
    "score_history=[]\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action,prob,val = agent.choose_action(state)\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        \n",
    "        score+=reward\n",
    "        n_step+=1\n",
    "        agent.remember(state,action,prob,val,reward,done)\n",
    "        state = next_state\n",
    "        if n_step %N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters+=1\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        agent.save_models()\n",
    "    print(f\"episode {ep} current score {score} avg score {avg_score} best_score {best_score}\")\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "90a4e5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 483.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n",
      "ep score = 500.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m----> 6\u001b[0m     action,prob,val \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     next_state,reward,done,_ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)   \n\u001b[1;32m      8\u001b[0m     score\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mreward\n",
      "Cell \u001b[0;32mIn[82], line 27\u001b[0m, in \u001b[0;36mAgent.choose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_action\u001b[39m(\u001b[38;5;28mself\u001b[39m,observation):\n\u001b[1;32m     25\u001b[0m     state \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mtensor([observation],dtype\u001b[38;5;241m=\u001b[39mT\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 27\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(state)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#TOCHECK what is the datatype action \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[46], line 30\u001b[0m, in \u001b[0;36mAgentNetwork.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     28\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#TOCHECK: what does categorical do\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/distributions/categorical.py:66\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     65\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCategorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/distributions/distribution.py:54\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[1;32m     53\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[0;32m---> 54\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/distributions/constraints.py:406\u001b[0m, in \u001b[0;36m_Simplex.check\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m--> 406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(value \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m ((\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m)\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action,prob,val = agent.choose_action(state)\n",
    "        next_state,reward,done,_ = env.step(action)   \n",
    "        score+=reward\n",
    "        n_step+=1 \n",
    "        state = next_state  \n",
    "        env.render()\n",
    "    print(f\"ep score = {score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
